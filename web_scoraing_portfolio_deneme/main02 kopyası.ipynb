{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5528f92-f973-4a97-bee5-895e01f101de",
   "metadata": {},
   "source": [
    "# \"Web Scraping with Selenium to Find a Job\" \n",
    "\n",
    "We will go through 3 main tasks to implement our project:\n",
    "\n",
    "Task 1: Importing libraries.\n",
    "\n",
    "Task 2: Define functions.\n",
    "\n",
    "Task 3: Web scraping with selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc42ec9-e367-454d-a1e7-290cfd658ce9",
   "metadata": {},
   "source": [
    "# Task 1 : Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7746b00f-5b5a-4866-831d-12b56c715dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfbfb6e-6233-4abe-9305-9b421869d22d",
   "metadata": {},
   "source": [
    "# Task 2 : Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f68a27f-889d-4c30-830e-1e1c54a38a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sleep function \n",
    "def sleep(x):\n",
    "    time.sleep(x)\n",
    "\n",
    "# Wait for a certain measure of time before throwing an exception\n",
    "def wait(x):\n",
    "    driver.implicitly_wait(x)\n",
    "\n",
    "# Click Function\n",
    "def click_bann_byID(ID):\n",
    "    actions = ActionChains(driver)\n",
    "    akzeptieren = driver.find_element(By.ID, ID)\n",
    "    actions.click(akzeptieren).perform()\n",
    "    wait(10)\n",
    "    sleep(0.5)\n",
    "\n",
    "# Find Elements Function\n",
    "def find_elements_HPCO(H,P,C,O):\n",
    "    header = driver.find_elements(By.CLASS_NAME, H)\n",
    "    publish = driver.find_elements(By.CLASS_NAME, P)\n",
    "    company = driver.find_elements(By.CLASS_NAME, C)\n",
    "    ort = driver.find_elements(By.CLASS_NAME, O) \n",
    "\n",
    "    list_header = [title.text for title in header]\n",
    "    list_publish = [pub.text for pub in publish]\n",
    "    list_company = [comp.text for comp in company]\n",
    "    list_ort = [o.text for o in ort]\n",
    "    return list_header, list_publish, list_company, list_ort\n",
    "\n",
    "# Scroll Down Function\n",
    "def scroll_down(x):\n",
    "    n=0\n",
    "    while n < x:\n",
    "        n+=1\n",
    "        actions.key_down(Keys.PAGE_DOWN).perform()\n",
    "        sleep(1.5)\n",
    "        actions.key_down(Keys.PAGE_DOWN).perform()\n",
    "        sleep(1.5)\n",
    "        actions.key_down(Keys.PAGE_DOWN).perform()\n",
    "        sleep(1.5)\n",
    "        actions.key_down(Keys.PAGE_UP).perform()\n",
    "        sleep(0.10)\n",
    "        actions.key_down(Keys.PAGE_DOWN).perform()\n",
    "        wait(10)\n",
    "        sleep(2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3feb03-1eeb-40e8-aa2a-48d4fea21a1a",
   "metadata": {},
   "source": [
    "# Web Scraping with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f24bb0e-db35-4c09-950f-541b8871c539",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 01 - STEPSTONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "962f38df-d454-44d7-afcc-8b1e7917ae67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- StepStone Job Searching Selenium Project ----------------------\n",
      "Create Driver\n",
      "Go to Website\n",
      "Banned\n",
      "h pc o\n",
      "Header 25 Publish 25 Company 24 Ort 25 Desc 25 Link 25\n",
      "Page Number : 2, DataFrame Shape : (25, 6)\n",
      "Page Number : 3, DataFrame Shape : (25, 6)\n",
      "Page Number : 4, DataFrame Shape : (25, 6)\n",
      "Page Number : 5, DataFrame Shape : (25, 6)\n",
      "Page Number : 6, DataFrame Shape : (25, 6)\n",
      "DataFrame End : (150, 6)\n",
      "Code Runned No Problem\n",
      "Time = 0:01:04.508044\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Title : Web Scrapping by Selenium \n",
    "Project Purpose: From StepStone scrap data for some Job Titels\n",
    "1 - Create Driver\n",
    "2 - Go to Website\n",
    "3 - Create ActionChain Object\n",
    "    3.1 - Click Banned \n",
    "4 - Take Title and Infos from Page\n",
    "    4.1 - Create Lists \n",
    "    4.2 - Create DataFrame\n",
    "    4.3 - Repeat Process\n",
    "    4.4 - Print and Save DataFrame\n",
    "'''\n",
    "\n",
    "print('---------------------- StepStone Job Searching Selenium Project ----------------------')\n",
    "start=datetime.now()  \n",
    "# Link Descriptions\n",
    "link_original_stepstone = 'https://www.stepstone.de/jobs/data-analyst/in-rietberg?radius=50&page=2'\n",
    "\n",
    "website_name = 'stepstone'\n",
    "job_name = 'Data Engineer'\n",
    "#job_name = 'Data Analyst'\n",
    "#job_name = 'Data Scientist'\n",
    "ort_ = 'Rietberg'\n",
    "radius = 100\n",
    "page_number = 1\n",
    "\n",
    "#  1 - Create Driver\n",
    "Path = '/Users/macbook/Desktop/projects/Github_Repositories/Portfolio Projects/02 - Web_Scraping_Job_Search/chromedriver'\n",
    "driver = webdriver.Chrome(Path)\n",
    "print('Create Driver')\n",
    "\n",
    "\n",
    "#  2 - Go to Website\n",
    "job_link = job_name.replace(' ', '-').lower()\n",
    "ort_link = ort_.lower()\n",
    "link = f'https://www.stepstone.de/jobs/{job_link}/in-{ort_link}?radius={radius}&page={page_number}&sort=2&action=sort_publish'\n",
    "\n",
    "driver.get(link)\n",
    "wait(5)\n",
    "sleep(2)\n",
    "print('Go to Website')\n",
    "#  3 - ActionChain Object created\n",
    "# 3.1 - Click Banned Accept\n",
    "ID = 'ccmgt_explicit_accept'\n",
    "click_bann_byID(ID)\n",
    "print('Banned')\n",
    "\n",
    "# 4 -  Take Infos from Page\n",
    "# 4.1 - Headers, Publish_Time ,Company, City\n",
    "H, P, C, O = ('res-29pyh9', 'res-rf8k2x', 'res-hbyqhf', 'res-1wf9en7')\n",
    "list_header, list_publish, list_company, list_ort = find_elements_HPCO(H,P,C,O)\n",
    "print('h pc o')\n",
    "# 4.2 - Description and Page number of results\n",
    "description = driver.find_elements(By.CLASS_NAME, 'res-17md5or')\n",
    "#result = driver.find_elements(By.CLASS_NAME, 'resultlist-1jx3vjx')\n",
    "\n",
    "\n",
    "# 4.3 - Get Links 'res-1dwe62q'\n",
    "list_link01  = driver.find_elements(By.CLASS_NAME, 'res-1dwe62q')\n",
    "list_link = [link.get_attribute('href') for link in list_link01]\n",
    "\n",
    "# 4.4 - Get Texts for each finding\n",
    "list_description = [des.text for des in description]\n",
    "print('Header',len(list_header), 'Publish',len(list_publish), 'Company',len(list_company[1:]), 'Ort',len(list_ort), 'Desc', len(list_description), 'Link',len(list_link))\n",
    "\n",
    "# 4.5 - Total Search Page Number\n",
    "#list_result = [res.text for res in result]\n",
    "#number_of_page = int(list_result[-2])\n",
    "#print(f'Number of Jobs Pages = {number_of_page}')\n",
    "\n",
    "# 4.6 - DataFrame df\n",
    "d = dict(job_title=np.array(list_header), publish=np.array(list_publish), company=np.array(list_company[1:]), city=np.array(list_ort) , description=np.array(list_description), link=np.array(list_link))\n",
    "df = pd.DataFrame.from_dict(d, orient='index')\n",
    "df = df.T\n",
    "\n",
    "number_of_page = 6\n",
    "# 4.7 Repeat Process for every Web Page\n",
    "while  page_number < number_of_page:\n",
    "    page_number+=1\n",
    "    \n",
    "    # 4.7.1 - Go to another page\n",
    "    link = f'https://www.stepstone.de/jobs/{job_link}/in-{ort_link}?radius={radius}&page={page_number}'\n",
    "    driver.get(link)\n",
    "    wait(5)\n",
    "    sleep(1.5)\n",
    "    \n",
    "    # 4.7.2 - Find the elements and get the Texts\n",
    "    list_header, list_publish, list_company, list_ort = find_elements_HPCO(H,P,C,O) \n",
    "    description = driver.find_elements(By.CLASS_NAME, 'res-17md5or')\n",
    "    list_description = [des.text for des in description]\n",
    "    list_link01  = driver.find_elements(By.CLASS_NAME, 'res-1dwe62q')\n",
    "    list_link = [link.get_attribute('href') for link in list_link01]\n",
    " \n",
    "    # 4.7.3 - Create new page Dataframe\n",
    "    d = dict(job_title=np.array(list_header), publish=np.array(list_publish), company=np.array(list_company[1:]), city=np.array(list_ort) , description=np.array(list_description), link=np.array(list_link))\n",
    "    df2 = pd.DataFrame.from_dict(d, orient='index')\n",
    "    df2 = df2.T\n",
    "    \n",
    "    # 4.7.4 - Concatenate the DataFrames\n",
    "    df = pd.concat([df,df2], axis=0, ignore_index=True)\n",
    "    print(f'Page Number : {page_number}, DataFrame Shape : {df2.shape}')\n",
    "\n",
    "\n",
    "# 5.1 - Save Data as csv \n",
    "print(f'DataFrame End : {df.shape}')\n",
    "df['website'] = website_name\n",
    "time_ = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "df['date'] = time_\n",
    "job_name2 = job_name.replace(' ', '_')\n",
    "df['search_title'] = job_name2\n",
    "\n",
    "path = '/Users/macbook/Desktop/projects/Github_Repositories/Portfolio Projects/02 - Web_Scraping_Job_Search/data'\n",
    "job_name3 = job_name.replace(' ', '-')\n",
    "time_ = datetime.today().strftime('%Y-%m-%d')\n",
    "df.to_csv(f'{path}/{job_name3}-{website_name}-{time_}.csv', index=False)\n",
    "\n",
    "# 6 - Quit\n",
    "end =datetime.now() \n",
    "print('Code Runned No Problem')\n",
    "print(f'Time = {end - start}')\n",
    "sleep(5)\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dab1674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>publish</th>\n",
       "      <th>company</th>\n",
       "      <th>city</th>\n",
       "      <th>description</th>\n",
       "      <th>link</th>\n",
       "      <th>website</th>\n",
       "      <th>date</th>\n",
       "      <th>search_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Junior Data Engineer (m/w/d)</td>\n",
       "      <td>vor 1 Tag</td>\n",
       "      <td>Amprion GmbH</td>\n",
       "      <td>Halle</td>\n",
       "      <td>Zur Unterstützung unserer Bereiche Anwendungsb...</td>\n",
       "      <td>https://www.stepstone.de/stellenangebote--Juni...</td>\n",
       "      <td>stepstone</td>\n",
       "      <td>2023-03-10 10:38:15</td>\n",
       "      <td>Data_Engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Expert (m/w/d)</td>\n",
       "      <td>vor 2 Tagen</td>\n",
       "      <td>Ratiodata SE</td>\n",
       "      <td>Dortmund</td>\n",
       "      <td>Abhängig von der Rolle verstärken Sie unser Te...</td>\n",
       "      <td>https://www.stepstone.de/stellenangebote--Data...</td>\n",
       "      <td>stepstone</td>\n",
       "      <td>2023-03-10 10:38:15</td>\n",
       "      <td>Data_Engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Client Engineer (m/w/d)</td>\n",
       "      <td>vor 2 Tagen</td>\n",
       "      <td>FALKE KGaA</td>\n",
       "      <td>Hannover (Wedemark), Münster, Duisburg</td>\n",
       "      <td>Ratiodata SE - Die Ratiodata SE zählt zu den g...</td>\n",
       "      <td>https://www.stepstone.de/stellenangebote--Clie...</td>\n",
       "      <td>stepstone</td>\n",
       "      <td>2023-03-10 10:38:15</td>\n",
       "      <td>Data_Engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Engineer DWH / BI (m/w/d)</td>\n",
       "      <td>vor 5 Tagen</td>\n",
       "      <td>DACHSER SE</td>\n",
       "      <td>Schmallenberg</td>\n",
       "      <td>Verstärken Sie das Team unseres Hauptsitzes am...</td>\n",
       "      <td>https://www.stepstone.de/stellenangebote--Data...</td>\n",
       "      <td>stepstone</td>\n",
       "      <td>2023-03-10 10:38:15</td>\n",
       "      <td>Data_Engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Integration Engineer (m/w/d) EDI</td>\n",
       "      <td>vor 6 Tagen</td>\n",
       "      <td>BabyOne Franchise- und Systemzentrale GmbH</td>\n",
       "      <td>Berlin, Hamburg, München, Köln, Frankfurt a. M...</td>\n",
       "      <td>Berlin, Hamburg, München, Köln, Frankfurt a. M...</td>\n",
       "      <td>https://www.stepstone.de/stellenangebote--Data...</td>\n",
       "      <td>stepstone</td>\n",
       "      <td>2023-03-10 10:38:15</td>\n",
       "      <td>Data_Engineer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               job_title      publish  \\\n",
       "0           Junior Data Engineer (m/w/d)    vor 1 Tag   \n",
       "1                    Data Expert (m/w/d)  vor 2 Tagen   \n",
       "2                Client Engineer (m/w/d)  vor 2 Tagen   \n",
       "3         Data Engineer DWH / BI (m/w/d)  vor 5 Tagen   \n",
       "4  Data Integration Engineer (m/w/d) EDI  vor 6 Tagen   \n",
       "\n",
       "                                      company  \\\n",
       "0                                Amprion GmbH   \n",
       "1                                Ratiodata SE   \n",
       "2                                  FALKE KGaA   \n",
       "3                                  DACHSER SE   \n",
       "4  BabyOne Franchise- und Systemzentrale GmbH   \n",
       "\n",
       "                                                city  \\\n",
       "0                                              Halle   \n",
       "1                                           Dortmund   \n",
       "2             Hannover (Wedemark), Münster, Duisburg   \n",
       "3                                      Schmallenberg   \n",
       "4  Berlin, Hamburg, München, Köln, Frankfurt a. M...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Zur Unterstützung unserer Bereiche Anwendungsb...   \n",
       "1  Abhängig von der Rolle verstärken Sie unser Te...   \n",
       "2  Ratiodata SE - Die Ratiodata SE zählt zu den g...   \n",
       "3  Verstärken Sie das Team unseres Hauptsitzes am...   \n",
       "4  Berlin, Hamburg, München, Köln, Frankfurt a. M...   \n",
       "\n",
       "                                                link    website  \\\n",
       "0  https://www.stepstone.de/stellenangebote--Juni...  stepstone   \n",
       "1  https://www.stepstone.de/stellenangebote--Data...  stepstone   \n",
       "2  https://www.stepstone.de/stellenangebote--Clie...  stepstone   \n",
       "3  https://www.stepstone.de/stellenangebote--Data...  stepstone   \n",
       "4  https://www.stepstone.de/stellenangebote--Data...  stepstone   \n",
       "\n",
       "                  date   search_title  \n",
       "0  2023-03-10 10:38:15  Data_Engineer  \n",
       "1  2023-03-10 10:38:15  Data_Engineer  \n",
       "2  2023-03-10 10:38:15  Data_Engineer  \n",
       "3  2023-03-10 10:38:15  Data_Engineer  \n",
       "4  2023-03-10 10:38:15  Data_Engineer  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "741a6c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abde48a-5765-4f02-81fd-b32ce46555e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 03 - LINKEDIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dbf2fbe-1a18-43d0-b510-6c6015993c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- Linkedin Job Searching Selenium Project ----------------------\n",
      "Header 48 Publish 48 Company 43 Ort 48 Desc 0 Link 43\n",
      "Number of Jobs Pages = ['Rietberg, Kuzey Ren-Vestfalya, Almanya konumunda 48 Business Analyst iş ilanı']\n",
      "DataFrame End : (48, 9)\n",
      "Code Runned No Problem\n",
      "Time = 0:00:39.972423\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>publish</th>\n",
       "      <th>company</th>\n",
       "      <th>city</th>\n",
       "      <th>description</th>\n",
       "      <th>link</th>\n",
       "      <th>website</th>\n",
       "      <th>date</th>\n",
       "      <th>search_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business Analyst (M/W/D)</td>\n",
       "      <td>2 gün önce</td>\n",
       "      <td>Positive Thinking Company</td>\n",
       "      <td>Bielefeld</td>\n",
       "      <td>None</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/business-ana...</td>\n",
       "      <td>linkedin</td>\n",
       "      <td>2023-01-23</td>\n",
       "      <td>Business_Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business Analyst (m/w/d) Sales</td>\n",
       "      <td>6 gün önce</td>\n",
       "      <td>Würth Industrie Service GmbH &amp; Co. KG</td>\n",
       "      <td>Bielefeld</td>\n",
       "      <td>None</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/business-ana...</td>\n",
       "      <td>linkedin</td>\n",
       "      <td>2023-01-23</td>\n",
       "      <td>Business_Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Analyst (m/w/d) Sales</td>\n",
       "      <td>6 gün önce</td>\n",
       "      <td>Würth Industrie Service GmbH &amp; Co. KG</td>\n",
       "      <td>Paderborn</td>\n",
       "      <td>None</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/business-ana...</td>\n",
       "      <td>linkedin</td>\n",
       "      <td>2023-01-23</td>\n",
       "      <td>Business_Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business Analyst agree21 , Junior oder Profi (...</td>\n",
       "      <td>2 ay önce</td>\n",
       "      <td>Beckmann &amp; Partner CONSULT</td>\n",
       "      <td>Bielefeld</td>\n",
       "      <td>None</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/business-ana...</td>\n",
       "      <td>linkedin</td>\n",
       "      <td>2023-01-23</td>\n",
       "      <td>Business_Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product Owner / Business Analyst (m/w/d) I E-C...</td>\n",
       "      <td>2 ay önce</td>\n",
       "      <td>Arvato Systems</td>\n",
       "      <td>Bielefeld</td>\n",
       "      <td>None</td>\n",
       "      <td>https://de.linkedin.com/jobs/view/product-owne...</td>\n",
       "      <td>linkedin</td>\n",
       "      <td>2023-01-23</td>\n",
       "      <td>Business_Analyst</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title     publish  \\\n",
       "0                           Business Analyst (M/W/D)  2 gün önce   \n",
       "1                     Business Analyst (m/w/d) Sales  6 gün önce   \n",
       "2                     Business Analyst (m/w/d) Sales  6 gün önce   \n",
       "3  Business Analyst agree21 , Junior oder Profi (...   2 ay önce   \n",
       "4  Product Owner / Business Analyst (m/w/d) I E-C...   2 ay önce   \n",
       "\n",
       "                                 company       city description  \\\n",
       "0              Positive Thinking Company  Bielefeld        None   \n",
       "1  Würth Industrie Service GmbH & Co. KG  Bielefeld        None   \n",
       "2  Würth Industrie Service GmbH & Co. KG  Paderborn        None   \n",
       "3             Beckmann & Partner CONSULT  Bielefeld        None   \n",
       "4                         Arvato Systems  Bielefeld        None   \n",
       "\n",
       "                                                link   website        date  \\\n",
       "0  https://de.linkedin.com/jobs/view/business-ana...  linkedin  2023-01-23   \n",
       "1  https://de.linkedin.com/jobs/view/business-ana...  linkedin  2023-01-23   \n",
       "2  https://de.linkedin.com/jobs/view/business-ana...  linkedin  2023-01-23   \n",
       "3  https://de.linkedin.com/jobs/view/business-ana...  linkedin  2023-01-23   \n",
       "4  https://de.linkedin.com/jobs/view/product-owne...  linkedin  2023-01-23   \n",
       "\n",
       "       search_title  \n",
       "0  Business_Analyst  \n",
       "1  Business_Analyst  \n",
       "2  Business_Analyst  \n",
       "3  Business_Analyst  \n",
       "4  Business_Analyst  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('---------------------- Linkedin Job Searching Selenium Project ----------------------')\n",
    "   \n",
    "job_name = 'Business Analyst'\n",
    "start=datetime.now()  \n",
    "# 0 Link Descriptions\n",
    "link_original = 'https://www.linkedin.com/jobs/search/?currentJobId=3199974140&distance=25&keywords=data%20analyst&location=Rietberg' \n",
    "\n",
    "website_name =  'linkedin'\n",
    "radius = 40\n",
    "page_number = 1\n",
    "\n",
    "#  1 - Create Driver\n",
    "Path = '/Users/macbook/Desktop/projects/Github_Repositories/Portfolio Projects/02 - Web_Scraping_Job_Search/chromedriver'\n",
    "driver = webdriver.Chrome(Path)\n",
    "\n",
    "#  2 - Go to Website\n",
    "job_link = job_name.replace(' ', '%20').lower()\n",
    "\n",
    "link2 = f'https://www.linkedin.com/jobs/search/?distance=25&keywords={job_link}&location={ort_}'\n",
    "driver.get(link2)\n",
    "wait(10)\n",
    "sleep(2)\n",
    "\n",
    "#  3 - ActionChain Object created\n",
    "# 3.1 - Click Banned Accept\n",
    "actions = ActionChains(driver)\n",
    "akzeptieren = driver.find_element(By.TAG_NAME, 'button')\n",
    "actions.click(akzeptieren).perform()\n",
    "wait(10)\n",
    "sleep(0.5)\n",
    "\n",
    "# 3.2 - Scroll Down Function\n",
    "scroll_down(4)\n",
    "\n",
    "# 4 -  Take Infos from Page\n",
    "# 4.1 - Headers, Company, City, Description\n",
    "H, P, C, O = 'base-search-card__title', 'job-search-card__listdate', 'hidden-nested-link', 'job-search-card__location'\n",
    "list_header, list_publish, list_company, list_ort = find_elements_HPCO(H,P,C,O)\n",
    "\n",
    "# 4.2 - Link Lists\n",
    "links = driver.find_elements(By.CLASS_NAME, 'base-card__full-link')\n",
    "list_link = [link.get_attribute('href') for link in links]\n",
    "\n",
    "print('Header',len(list_header), 'Publish',len(list_publish), 'Company',len(list_company), 'Ort',len(list_ort), 'Desc', len(list_description), 'Link',len(list_link))\n",
    "\n",
    "# 4.3 - Total Search Page Number\n",
    "result = driver.find_elements(By.CLASS_NAME, 'results-context-header__context')\n",
    "list_result = [res.text for res in result]\n",
    "print(f'Number of Jobs Pages = {list_result}')\n",
    "\n",
    "\n",
    "# 4.4 - DataFrame df\n",
    "d = dict(job_title=np.array(list_header), publish=np.array(list_publish), company=np.array(list_company), city=np.array(list_ort) , description=np.array(list_description), link=np.array(list_link))\n",
    "df = pd.DataFrame.from_dict(d, orient='index')\n",
    "df = df.T\n",
    "df['description'] = None\n",
    "df['website'] = website_name\n",
    "df['date'] = time_\n",
    "job_name2 = job_name.replace(' ', '_')\n",
    "df['search_title'] = job_name2\n",
    "\n",
    "# 5.1 - Save Data as csv \n",
    "print(f'DataFrame End : {df.shape}')\n",
    "df.loc[df.website =='linkedin', 'city'] = df.loc[df.website =='linkedin', 'city'].str.replace(', Kuzey Ren-Vestfalya, Almanya', '')\n",
    "df.to_csv(f'{path}/{job_name3}-{time_}.csv', mode='a', index=False, header=False)\n",
    "\n",
    "# 4.5 Quit\n",
    "end =datetime.now() \n",
    "print('Code Runned No Problem')\n",
    "print(f'Time = {end - start}')\n",
    "sleep(5)\n",
    "driver.quit()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed69c2b-d83a-44d8-88d8-a8268a0f64a5",
   "metadata": {},
   "source": [
    "# Stepstone, Jobware and Linkedin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d6a02e-7230-4c2a-958d-18e2f26459cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- StepStone Job Searching Selenium Project ----------------------\n",
      "1 Business Analyst\n",
      "Header 25 Publish 25 Company 25 Ort 25 Desc 25 Link 25\n",
      "Number of Jobs Pages = 3\n",
      "Page Number : 2, DataFrame Shape : (25, 6)\n",
      "Page Number : 3, DataFrame Shape : (8, 6)\n",
      "DataFrame End : (58, 6)\n",
      "Code Runned No Problem\n",
      "Time = 0:01:06.876843\n",
      "---------------------- Jobware Job Searching Selenium Project ----------------------\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=109.0.5414.87)\nStacktrace:\n0   chromedriver                        0x0000000105b937a8 chromedriver + 4896680\n1   chromedriver                        0x0000000105b12db3 chromedriver + 4369843\n2   chromedriver                        0x000000010575ca37 chromedriver + 477751\n3   chromedriver                        0x0000000105731fb5 chromedriver + 303029\n4   chromedriver                        0x00000001057cc77f chromedriver + 935807\n5   chromedriver                        0x00000001057e1923 chromedriver + 1022243\n6   chromedriver                        0x00000001057c7233 chromedriver + 913971\n7   chromedriver                        0x000000010579145c chromedriver + 693340\n8   chromedriver                        0x00000001057929de chromedriver + 698846\n9   chromedriver                        0x0000000105b61ace chromedriver + 4692686\n10  chromedriver                        0x0000000105b65da1 chromedriver + 4709793\n11  chromedriver                        0x0000000105b665aa chromedriver + 4711850\n12  chromedriver                        0x0000000105b6d62f chromedriver + 4740655\n13  chromedriver                        0x0000000105b66caa chromedriver + 4713642\n14  chromedriver                        0x0000000105b3a992 chromedriver + 4532626\n15  chromedriver                        0x0000000105b853c8 chromedriver + 4838344\n16  chromedriver                        0x0000000105b85545 chromedriver + 4838725\n17  chromedriver                        0x0000000105b9b6ef chromedriver + 4929263\n18  libsystem_pthread.dylib             0x00007fff204ff950 _pthread_start + 224\n19  libsystem_pthread.dylib             0x00007fff204fb47b thread_start + 15\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# 4 -  Take Infos from Page\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# 4.1 - Headers, Company, City, Description\u001b[39;00m\n\u001b[1;32m    161\u001b[0m H, P, C, O \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompany\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 162\u001b[0m list_header, list_publish, list_company, list_ort \u001b[38;5;241m=\u001b[39m \u001b[43mfind_elements_HPCO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43mO\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m description \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    164\u001b[0m list_description \u001b[38;5;241m=\u001b[39m [des\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m des \u001b[38;5;129;01min\u001b[39;00m description]\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mfind_elements_HPCO\u001b[0;34m(H, P, C, O)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_elements_HPCO\u001b[39m(H,P,C,O):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m website_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjobware\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m         header \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTAG_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m         header \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, H)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:889\u001b[0m, in \u001b[0;36mWebDriver.find_elements\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    885\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m value\n\u001b[1;32m    887\u001b[0m \u001b[38;5;66;03m# Return empty list if driver returns null\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# See https://github.com/SeleniumHQ/selenium/issues/4555\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:429\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    427\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(\n\u001b[1;32m    431\u001b[0m         response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/selenium/webdriver/remote/errorhandler.py:243\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    241\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=109.0.5414.87)\nStacktrace:\n0   chromedriver                        0x0000000105b937a8 chromedriver + 4896680\n1   chromedriver                        0x0000000105b12db3 chromedriver + 4369843\n2   chromedriver                        0x000000010575ca37 chromedriver + 477751\n3   chromedriver                        0x0000000105731fb5 chromedriver + 303029\n4   chromedriver                        0x00000001057cc77f chromedriver + 935807\n5   chromedriver                        0x00000001057e1923 chromedriver + 1022243\n6   chromedriver                        0x00000001057c7233 chromedriver + 913971\n7   chromedriver                        0x000000010579145c chromedriver + 693340\n8   chromedriver                        0x00000001057929de chromedriver + 698846\n9   chromedriver                        0x0000000105b61ace chromedriver + 4692686\n10  chromedriver                        0x0000000105b65da1 chromedriver + 4709793\n11  chromedriver                        0x0000000105b665aa chromedriver + 4711850\n12  chromedriver                        0x0000000105b6d62f chromedriver + 4740655\n13  chromedriver                        0x0000000105b66caa chromedriver + 4713642\n14  chromedriver                        0x0000000105b3a992 chromedriver + 4532626\n15  chromedriver                        0x0000000105b853c8 chromedriver + 4838344\n16  chromedriver                        0x0000000105b85545 chromedriver + 4838725\n17  chromedriver                        0x0000000105b9b6ef chromedriver + 4929263\n18  libsystem_pthread.dylib             0x00007fff204ff950 _pthread_start + 224\n19  libsystem_pthread.dylib             0x00007fff204fb47b thread_start + 15\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Title : Web Scrapping by Selenium \n",
    "Project Purpose: From StepStone scrap data for some Job Titels\n",
    "1 - Create Driver\n",
    "2 - Go to Website\n",
    "3 - Create ActionChain Object\n",
    "    3.1 - Click Banned \n",
    "4 - Take Title and Infos from Page\n",
    "    4.1 - Create Lists \n",
    "    4.2 - Create DataFrame\n",
    "    4.3 - Repeat Process\n",
    "    4.4 - Print and Save DataFrame\n",
    "'''\n",
    "\n",
    "job_list = ['Business Analyst', 'Data Scientist'] #, 'Data Analyst' \n",
    "n=0\n",
    "start_01=datetime.now()\n",
    "while n<len(job_list):\n",
    "    print('---------------------- StepStone Job Searching Selenium Project ----------------------')\n",
    "    start=datetime.now()  \n",
    "    n+=1\n",
    "    # Link Descriptions\n",
    "    link_original_stepstone = 'https://www.stepstone.de/jobs/data-analyst/in-rietberg?radius=50&page=2'\n",
    "\n",
    "    website_name = 'stepstone'\n",
    "    job_name = job_list[n-1] \n",
    "    print(n, job_name)\n",
    "    ort_ = 'Rietberg'\n",
    "    radius = 50\n",
    "    page_number = 1\n",
    "\n",
    "    #  1 - Create Driver\n",
    "    Path = '/Users/macbook/Desktop/projects/Github_Repositories/Portfolio Projects/02 - Web_Scraping_Job_Search/chromedriver'\n",
    "    driver = webdriver.Chrome(Path)\n",
    "\n",
    "    #  2 - Go to Website\n",
    "    job_link = job_name.replace(' ', '-').lower()\n",
    "    ort_link = ort_.lower()\n",
    "    link = f'https://www.stepstone.de/jobs/{job_link}/in-{ort_link}?radius={radius}&page={page_number}'\n",
    "\n",
    "    driver.get(link)\n",
    "    wait(10)\n",
    "    sleep(2)\n",
    "\n",
    "    #  3 - ActionChain Object created\n",
    "    # 3.1 - Click Banned Accept\n",
    "    ID = 'ccmgt_explicit_accept'\n",
    "    click_bann_byID(ID)\n",
    "\n",
    "\n",
    "    # 4 -  Take Infos from Page\n",
    "    # 4.1 - Headers, Publish_Time ,Company, City\n",
    "    H, P, C, O = 'resultlist-1uvdp0v', 'resultlist-w7zbt7', 'resultlist-1va1dj8', 'resultlist-suri3e'\n",
    "    list_header, list_publish, list_company, list_ort = find_elements_HPCO(H,P,C,O)\n",
    "\n",
    "    # 4.2 - Description and Page number of results\n",
    "    description = driver.find_elements(By.CLASS_NAME, 'resultlist-1fp8oay')\n",
    "    result = driver.find_elements(By.CLASS_NAME, 'resultlist-1jx3vjx')\n",
    "\n",
    "\n",
    "    # 4.3 - Get Links\n",
    "    header = driver.find_elements(By.CLASS_NAME, H)\n",
    "    list_link = [link.get_attribute('href') for link in header]\n",
    "\n",
    "    # 4.4 - Get Texts for each finding\n",
    "    list_description = [des.text for des in description]\n",
    "    print('Header',len(list_header), 'Publish',len(list_publish), 'Company',len(list_company[1:]), 'Ort',len(list_ort), 'Desc', len(list_description), 'Link',len(list_link))\n",
    "\n",
    "    # 4.5 - Total Search Page Number\n",
    "    list_result = [res.text for res in result]\n",
    "    number_of_page = int(list_result[-2])\n",
    "    print(f'Number of Jobs Pages = {number_of_page}')\n",
    "\n",
    "    # 4.6 - DataFrame df\n",
    "    d = dict(job_title=np.array(list_header), publish=np.array(list_publish), company=np.array(list_company[1:]), city=np.array(list_ort) , description=np.array(list_description), link=np.array(list_link))\n",
    "    df = pd.DataFrame.from_dict(d, orient='index')\n",
    "    df = df.T\n",
    "\n",
    "\n",
    "    # 4.7 Repeat Process for every Web Page\n",
    "    while  page_number < number_of_page:\n",
    "        page_number+=1\n",
    "\n",
    "        # 4.7.1 - Go to another page\n",
    "        link = f'https://www.stepstone.de/jobs/{job_link}/in-{ort_link}?radius={radius}&page={page_number}'\n",
    "        driver.get(link)\n",
    "        wait(10)\n",
    "        sleep(1.5)\n",
    "\n",
    "        # 4.7.2 - Find the elements and get the Texts\n",
    "        list_header, list_publish, list_company, list_ort = find_elements_HPCO(H,P,C,O) \n",
    "        description = driver.find_elements(By.CLASS_NAME, 'resultlist-1pq4x2u')\n",
    "        list_description = [des.text for des in description]\n",
    "        header = driver.find_elements(By.CLASS_NAME, H)\n",
    "        list_link = [link.get_attribute('href') for link in header]\n",
    "\n",
    "        # 4.7.3 - Create new page Dataframe\n",
    "        d = dict(job_title=np.array(list_header), publish=np.array(list_publish), company=np.array(list_company[1:]), city=np.array(list_ort) , description=np.array(list_description), link=np.array(list_link))\n",
    "        df2 = pd.DataFrame.from_dict(d, orient='index')\n",
    "        df2 = df2.T\n",
    "\n",
    "        # 4.7.4 - Concatenate the DataFrames\n",
    "        df = pd.concat([df,df2], axis=0, ignore_index=True)\n",
    "        print(f'Page Number : {page_number}, DataFrame Shape : {df2.shape}')\n",
    "\n",
    "\n",
    "    # 5.1 - Save Data as csv \n",
    "    print(f'DataFrame End : {df.shape}')\n",
    "    df['website'] = website_name\n",
    "    time_ = datetime.today().strftime('%Y-%m-%d')\n",
    "    df['date'] = time_\n",
    "    job_name2 = job_name.replace(' ', '_')\n",
    "    df['search_title'] = job_name2\n",
    "\n",
    "    path = '/Users/macbook/Desktop/projects/Github_Repositories/Portfolio Projects/02 - Web_Scraping_Job_Search/data'\n",
    "    job_name3 = job_name.replace(' ', '-')\n",
    "    time_ = datetime.today().strftime('%Y-%m-%d')\n",
    "    df.to_csv(f'{path}/{job_name3}-{time_}.csv', index=False)\n",
    "\n",
    "    # 6 - Quit\n",
    "    end =datetime.now() \n",
    "    print('Code Runned No Problem')\n",
    "    print(f'Time = {end - start}')\n",
    "    sleep(1)\n",
    "    driver.quit()\n",
    "\n",
    "    print('---------------------- Jobware Job Searching Selenium Project ----------------------')\n",
    "\n",
    "    start=datetime.now()  \n",
    "    # 0 Link Descriptions\n",
    "    link_original = 'https://www.jobware.de/jobsuche?jw_jobname=data%20analyst&jw_jobort=333**%20Rietberg&jw_ort_distance=50'\n",
    "\n",
    "    website_name = 'jobware'\n",
    "    radius = 50\n",
    "    page_number = 0\n",
    "\n",
    "    #  1 - Create Driver\n",
    "    Path = '/Users/macbook/Desktop/projects/Github_Repositories/Portfolio Projects/02 - Web_Scraping_Job_Search/chromedriver'\n",
    "    driver = webdriver.Chrome(Path)\n",
    "\n",
    "    #  2 - Go to Website\n",
    "    job_link = job_name.replace(' ', '%20').lower()\n",
    "    ort_link = ort_.capitalize()\n",
    "    link = f'https://www.jobware.de/jobsuche?jw_jobname={job_link}&jw_jobort=333**%20{ort_}&jw_ort_distance={radius}'\n",
    "\n",
    "    driver.get(link)\n",
    "    wait(10)\n",
    "    sleep(2)\n",
    "\n",
    "    #  3 - ActionChain Object created\n",
    "    # 3.1 - Click Banned Accept\n",
    "    actions = ActionChains(driver)\n",
    "    akzeptieren = driver.find_element(By.XPATH, '/html/body/div[1]/div/div[3]/div[2]/button')\n",
    "    actions.click(akzeptieren).perform()\n",
    "    wait(10)\n",
    "    sleep(0.5)\n",
    "\n",
    "\n",
    "    # 4 -  Take Infos from Page\n",
    "    # 4.1 - Headers, Company, City, Description\n",
    "    H, P, C, O = 'h2', 'date', 'company', 'location'\n",
    "    list_header, list_publish, list_company, list_ort = find_elements_HPCO(H,P,C,O)\n",
    "    description = driver.find_elements(By.CLASS_NAME, 'task')\n",
    "    list_description = [des.text for des in description]\n",
    "\n",
    "    links = driver.find_elements(By.CLASS_NAME, 'job')\n",
    "    list_link = [link.get_attribute('href') for link in links]\n",
    "\n",
    "    print('Header',len(list_header), 'Publish',len(list_publish), 'Company',len(list_company), 'Ort',len(list_ort), 'Desc', len(list_description), 'Link',len(list_link))\n",
    "\n",
    "    # 4.2 - Total Search Page Number\n",
    "    result = driver.find_elements(By.CLASS_NAME, 'result-sort')\n",
    "    list_result = [res.text for res in result]\n",
    "    print(list_result)\n",
    "\n",
    "    # 4.3 - DataFrame df\n",
    "    d = dict(job_title=np.array(list_header), publish=np.array(list_publish), company=np.array(list_company), city=np.array(list_ort) , description=np.array(list_description), link=np.array(list_link))\n",
    "    df = pd.DataFrame.from_dict(d, orient='index')\n",
    "    df = df.T\n",
    "\n",
    "    # 5.1 - Save Data as csv\n",
    "    print(f'DataFrame End : {df.shape}')\n",
    "    df['website'] = website_name\n",
    "    time_ = datetime.today().strftime('%Y-%m-%d')\n",
    "    df['date'] = time_\n",
    "    job_name2 = job_name.replace(' ', '_')\n",
    "    df['search_title'] = job_name2\n",
    "\n",
    "\n",
    "\n",
    "    df.to_csv(f'{path}/{job_name3}-{time_}.csv', mode='a', index=False, header=False)\n",
    "\n",
    "    # 6.1 Quit\n",
    "    end =datetime.now() \n",
    "    print('Code Runned No Problem')\n",
    "    print(f'Time = {end - start}')\n",
    "    sleep(5)\n",
    "    driver.quit()\n",
    "\n",
    "    print('---------------------- Linkedin Job Searching Selenium Project ----------------------')\n",
    "\n",
    "\n",
    "    start=datetime.now()  \n",
    "    # 0 Link Descriptions\n",
    "    link_original = 'https://www.linkedin.com/jobs/search/?currentJobId=3199974140&distance=25&keywords=data%20analyst&location=Rietberg' \n",
    "\n",
    "    website_name =  'linkedin'\n",
    "    radius = 40\n",
    "    page_number = 1\n",
    "\n",
    "    #  1 - Create Driver\n",
    "    Path = '/Users/macbook/Desktop/projects/Github_Repositories/Portfolio Projects/02 - Web_Scraping_Job_Search/chromedriver'\n",
    "    driver = webdriver.Chrome(Path)\n",
    "\n",
    "    #  2 - Go to Website\n",
    "    job_link = job_name.replace(' ', '%20').lower()\n",
    "\n",
    "    link2 = f'https://www.linkedin.com/jobs/search/?distance=25&keywords={job_link}&location={ort_}'\n",
    "    driver.get(link2)\n",
    "    wait(10)\n",
    "    sleep(2)\n",
    "\n",
    "    #  3 - ActionChain Object created\n",
    "    # 3.1 - Click Banned Accept\n",
    "    actions = ActionChains(driver)\n",
    "    akzeptieren = driver.find_element(By.TAG_NAME, 'button')\n",
    "    actions.click(akzeptieren).perform()\n",
    "    wait(10)\n",
    "    sleep(0.5)\n",
    "\n",
    "    # 3.2 - Scroll Down Function\n",
    "    scroll_down(4)\n",
    "\n",
    "    # 4 -  Take Infos from Page\n",
    "    # 4.1 - Headers, Company, City, Description\n",
    "    H, P, C, O = 'base-search-card__title', 'job-search-card__listdate', 'hidden-nested-link', 'job-search-card__location'\n",
    "    list_header, list_publish, list_company, list_ort = find_elements_HPCO(H,P,C,O)\n",
    "\n",
    "    # 4.2 - Link Lists\n",
    "    links = driver.find_elements(By.CLASS_NAME, 'base-card__full-link')\n",
    "    list_link = [link.get_attribute('href') for link in links]\n",
    "\n",
    "    print('Header',len(list_header), 'Publish',len(list_publish), 'Company',len(list_company), 'Ort',len(list_ort), 'Desc', len(list_description), 'Link',len(list_link))\n",
    "\n",
    "    # 4.3 - Total Search Page Number\n",
    "    result = driver.find_elements(By.CLASS_NAME, 'results-context-header__context')\n",
    "    list_result = [res.text for res in result]\n",
    "    print(f'Number of Jobs Pages = {list_result}')\n",
    "\n",
    "\n",
    "    # 4.4 - DataFrame df\n",
    "    d = dict(job_title=np.array(list_header), publish=np.array(list_publish), company=np.array(list_company), city=np.array(list_ort) , description=np.array(list_description), link=np.array(list_link))\n",
    "    df = pd.DataFrame.from_dict(d, orient='index')\n",
    "    df = df.T\n",
    "    df['description'] = None\n",
    "    df['website'] = website_name\n",
    "    df['date'] = time_\n",
    "    job_name2 = job_name.replace(' ', '_')\n",
    "    df['search_title'] = job_name2\n",
    "\n",
    "    # 5.1 - Save Data as csv \n",
    "    print(f'DataFrame End : {df.shape}')\n",
    "    df.loc[df.website =='linkedin', 'city'] = df.loc[df.website =='linkedin', 'city'].str.replace(', Kuzey Ren-Vestfalya, Almanya', '')\n",
    "    df.to_csv(f'{path}/{job_name3}-{time_}.csv', mode='a', index=False, header=False)\n",
    "\n",
    "    # 4.5 Quit\n",
    "    end =datetime.now() \n",
    "    print('Code Runned No Problem')\n",
    "    print(f'Time = {end - start}')\n",
    "    sleep(5)\n",
    "    driver.quit()\n",
    "\n",
    "    df.head()\n",
    "    \n",
    "end_01 =datetime.now()\n",
    "print(f'Total Time: {end_01 - start_01}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eb52cb-2201-4d56-9264-60d9fe59efae",
   "metadata": {},
   "source": [
    "# Stepstone , Linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83f60644-b99f-4dd7-8e44-6d99be5446f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- StepStone Job Searching Selenium Project ----------------------\n",
      "1 Business Analyst\n",
      "Header 25 Publish 25 Company 25 Ort 25 Desc 25 Link 25\n",
      "Number of Jobs Pages = 3\n",
      "Page Number : 2, DataFrame Shape : (25, 6)\n",
      "Page Number : 3, DataFrame Shape : (6, 6)\n",
      "DataFrame End : (56, 6)\n",
      "Code Runned No Problem\n",
      "Time = 0:01:02.657421\n",
      "---------------------- Linkedin Job Searching Selenium Project ----------------------\n",
      "Header 25 Publish 25 Company 23 Ort 25 Desc 0 Link 23\n",
      "Number of Jobs Pages = ['Rietberg, Kuzey Ren-Vestfalya, Almanya konumunda 47 Business Analyst iş ilanı']\n",
      "DataFrame End : (25, 9)\n",
      "Code Runned No Problem\n",
      "Time = 0:00:37.934671\n",
      "---------------------- StepStone Job Searching Selenium Project ----------------------\n",
      "2 Data Scientist\n",
      "Header 25 Publish 25 Company 25 Ort 25 Desc 25 Link 25\n",
      "Number of Jobs Pages = 2\n",
      "Page Number : 2, DataFrame Shape : (1, 6)\n",
      "DataFrame End : (26, 6)\n",
      "Code Runned No Problem\n",
      "Time = 0:00:44.833504\n",
      "---------------------- Linkedin Job Searching Selenium Project ----------------------\n",
      "Header 50 Publish 50 Company 46 Ort 50 Desc 0 Link 46\n",
      "Number of Jobs Pages = ['Rietberg, Kuzey Ren-Vestfalya, Almanya konumunda 803 Data Scientist iş ilanı (27 yeni)']\n",
      "DataFrame End : (50, 9)\n",
      "Code Runned No Problem\n",
      "Time = 0:00:39.039891\n",
      "Total Time: 0:03:17.380862\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Title : Web Scrapping by Selenium \n",
    "Project Purpose: From StepStone scrap data for some Job Titels\n",
    "1 - Create Driver\n",
    "2 - Go to Website\n",
    "3 - Create ActionChain Object\n",
    "    3.1 - Click Banned \n",
    "4 - Take Title and Infos from Page\n",
    "    4.1 - Create Lists \n",
    "    4.2 - Create DataFrame\n",
    "    4.3 - Repeat Process\n",
    "    4.4 - Print and Save DataFrame\n",
    "'''\n",
    "\n",
    "job_list = ['Business Analyst', 'Data Scientist'] #, 'Data Analyst' \n",
    "n=0\n",
    "start_01=datetime.now()\n",
    "while n<len(job_list):\n",
    "    print('---------------------- StepStone Job Searching Selenium Project ----------------------')\n",
    "    start=datetime.now()  \n",
    "    n+=1\n",
    "    # Link Descriptions\n",
    "    link_original_stepstone = 'https://www.stepstone.de/jobs/data-analyst/in-rietberg?radius=50&page=2'\n",
    "\n",
    "    website_name = 'stepstone'\n",
    "    job_name = job_list[n-1] \n",
    "    print(n, job_name)\n",
    "    ort_ = 'Rietberg'\n",
    "    radius = 50\n",
    "    page_number = 1\n",
    "\n",
    "    #  1 - Create Driver\n",
    "    Path = '/Users/macbook/Desktop/projects/Github_Repositories/Portfolio Projects/02 - Web_Scraping_Job_Search/chromedriver'\n",
    "    driver = webdriver.Chrome(Path)\n",
    "\n",
    "    #  2 - Go to Website\n",
    "    job_link = job_name.replace(' ', '-').lower()\n",
    "    ort_link = ort_.lower()\n",
    "    link = f'https://www.stepstone.de/jobs/{job_link}/in-{ort_link}?radius={radius}&page={page_number}'\n",
    "\n",
    "    driver.get(link)\n",
    "    wait(10)\n",
    "    sleep(2)\n",
    "\n",
    "    #  3 - ActionChain Object created\n",
    "    # 3.1 - Click Banned Accept\n",
    "    ID = 'ccmgt_explicit_accept'\n",
    "    click_bann_byID(ID)\n",
    "\n",
    "\n",
    "    # 4 -  Take Infos from Page\n",
    "    # 4.1 - Headers, Publish_Time ,Company, City\n",
    "    H, P, C, O = 'resultlist-1uvdp0v', 'resultlist-w7zbt7', 'resultlist-1va1dj8', 'resultlist-suri3e'\n",
    "    list_header, list_publish, list_company, list_ort = find_elements_HPCO(H,P,C,O)\n",
    "\n",
    "    # 4.2 - Description and Page number of results\n",
    "    description = driver.find_elements(By.CLASS_NAME, 'resultlist-1fp8oay')\n",
    "    result = driver.find_elements(By.CLASS_NAME, 'resultlist-1jx3vjx')\n",
    "\n",
    "\n",
    "    # 4.3 - Get Links\n",
    "    header = driver.find_elements(By.CLASS_NAME, H)\n",
    "    list_link = [link.get_attribute('href') for link in header]\n",
    "\n",
    "    # 4.4 - Get Texts for each finding\n",
    "    list_description = [des.text for des in description]\n",
    "    print('Header',len(list_header), 'Publish',len(list_publish), 'Company',len(list_company[1:]), 'Ort',len(list_ort), 'Desc', len(list_description), 'Link',len(list_link))\n",
    "\n",
    "    # 4.5 - Total Search Page Number\n",
    "    list_result = [res.text for res in result]\n",
    "    number_of_page = int(list_result[-2])\n",
    "    print(f'Number of Jobs Pages = {number_of_page}')\n",
    "\n",
    "    # 4.6 - DataFrame df\n",
    "    d = dict(job_title=np.array(list_header), publish=np.array(list_publish), company=np.array(list_company[1:]), city=np.array(list_ort) , description=np.array(list_description), link=np.array(list_link))\n",
    "    df = pd.DataFrame.from_dict(d, orient='index')\n",
    "    df = df.T\n",
    "\n",
    "\n",
    "    # 4.7 Repeat Process for every Web Page\n",
    "    while  page_number < number_of_page:\n",
    "        page_number+=1\n",
    "\n",
    "        # 4.7.1 - Go to another page\n",
    "        link = f'https://www.stepstone.de/jobs/{job_link}/in-{ort_link}?radius={radius}&page={page_number}'\n",
    "        driver.get(link)\n",
    "        wait(10)\n",
    "        sleep(0.5)\n",
    "\n",
    "        # 4.7.2 - Find the elements and get the Texts\n",
    "        list_header, list_publish, list_company, list_ort = find_elements_HPCO(H,P,C,O) \n",
    "        description = driver.find_elements(By.CLASS_NAME, 'resultlist-1pq4x2u')\n",
    "        list_description = [des.text for des in description]\n",
    "        header = driver.find_elements(By.CLASS_NAME, H)\n",
    "        list_link = [link.get_attribute('href') for link in header]\n",
    "\n",
    "        # 4.7.3 - Create new page Dataframe\n",
    "        d = dict(job_title=np.array(list_header), publish=np.array(list_publish), company=np.array(list_company[1:]), city=np.array(list_ort) , description=np.array(list_description), link=np.array(list_link))\n",
    "        df2 = pd.DataFrame.from_dict(d, orient='index')\n",
    "        df2 = df2.T\n",
    "\n",
    "        # 4.7.4 - Concatenate the DataFrames\n",
    "        df = pd.concat([df,df2], axis=0, ignore_index=True)\n",
    "        print(f'Page Number : {page_number}, DataFrame Shape : {df2.shape}')\n",
    "\n",
    "\n",
    "    # 5.1 - Save Data as csv \n",
    "    print(f'DataFrame End : {df.shape}')\n",
    "    df['website'] = website_name\n",
    "    time_ = datetime.today().strftime('%Y-%m-%d')\n",
    "    df['date'] = time_\n",
    "    job_name2 = job_name.replace(' ', '_')\n",
    "    df['search_title'] = job_name2\n",
    "\n",
    "    path = '/Users/macbook/Desktop/projects/Github_Repositories/Portfolio Projects/02 - Web_Scraping_Job_Search/data'\n",
    "    job_name3 = job_name.replace(' ', '-')\n",
    "    time_ = datetime.today().strftime('%Y-%m-%d')\n",
    "    df.to_csv(f'{path}/{job_name3}-{time_}.csv', index=False)\n",
    "\n",
    "    # 6 - Quit\n",
    "    end =datetime.now() \n",
    "    print('Code Runned No Problem')\n",
    "    print(f'Time = {end - start}')\n",
    "    sleep(1)\n",
    "    driver.quit()\n",
    "\n",
    "    \n",
    "    print('---------------------- Linkedin Job Searching Selenium Project ----------------------')\n",
    "\n",
    "\n",
    "    start=datetime.now()  \n",
    "    # 0 Link Descriptions\n",
    "    link_original = 'https://www.linkedin.com/jobs/search/?currentJobId=3199974140&distance=25&keywords=data%20analyst&location=Rietberg' \n",
    "\n",
    "    website_name =  'linkedin'\n",
    "    radius = 40\n",
    "    page_number = 1\n",
    "\n",
    "    #  1 - Create Driver\n",
    "    Path = '/Users/macbook/Desktop/projects/Github_Repositories/Portfolio Projects/02 - Web_Scraping_Job_Search/chromedriver'\n",
    "    driver = webdriver.Chrome(Path)\n",
    "\n",
    "    #  2 - Go to Website\n",
    "    job_link = job_name.replace(' ', '%20').lower()\n",
    "\n",
    "    link2 = f'https://www.linkedin.com/jobs/search/?distance=25&keywords={job_link}&location={ort_}'\n",
    "    driver.get(link2)\n",
    "    wait(10)\n",
    "    sleep(2)\n",
    "\n",
    "    #  3 - ActionChain Object created\n",
    "    # 3.1 - Click Banned Accept\n",
    "    actions = ActionChains(driver)\n",
    "    akzeptieren = driver.find_element(By.TAG_NAME, 'button')\n",
    "    actions.click(akzeptieren).perform()\n",
    "    wait(10)\n",
    "    sleep(0.5)\n",
    "\n",
    "    # 3.2 - Scroll Down Function\n",
    "    scroll_down(4)\n",
    "\n",
    "    # 4 -  Take Infos from Page\n",
    "    # 4.1 - Headers, Company, City, Description\n",
    "    H, P, C, O = 'base-search-card__title', 'job-search-card__listdate', 'hidden-nested-link', 'job-search-card__location'\n",
    "    list_header, list_publish, list_company, list_ort = find_elements_HPCO(H,P,C,O)\n",
    "\n",
    "    # 4.2 - Link Lists\n",
    "    links = driver.find_elements(By.CLASS_NAME, 'base-card__full-link')\n",
    "    list_link = [link.get_attribute('href') for link in links]\n",
    "\n",
    "    print('Header',len(list_header), 'Publish',len(list_publish), 'Company',len(list_company), 'Ort',len(list_ort), 'Desc', len(list_description), 'Link',len(list_link))\n",
    "\n",
    "    # 4.3 - Total Search Page Number\n",
    "    result = driver.find_elements(By.CLASS_NAME, 'results-context-header__context')\n",
    "    list_result = [res.text for res in result]\n",
    "    print(f'Number of Jobs Pages = {list_result}')\n",
    "\n",
    "\n",
    "    # 4.4 - DataFrame df\n",
    "    d = dict(job_title=np.array(list_header), publish=np.array(list_publish), company=np.array(list_company), city=np.array(list_ort) , description=np.array(list_description), link=np.array(list_link))\n",
    "    df = pd.DataFrame.from_dict(d, orient='index')\n",
    "    df = df.T\n",
    "    df['description'] = None\n",
    "    df['website'] = website_name\n",
    "    df['date'] = time_\n",
    "    job_name2 = job_name.replace(' ', '_')\n",
    "    df['search_title'] = job_name2\n",
    "\n",
    "    # 5.1 - Save Data as csv \n",
    "    print(f'DataFrame End : {df.shape}')\n",
    "    df.loc[df.website =='linkedin', 'city'] = df.loc[df.website =='linkedin', 'city'].str.replace(', Kuzey Ren-Vestfalya, Almanya', '')\n",
    "    df.to_csv(f'{path}/{job_name3}-{website_name}-{time_}.csv', mode='a', index=False, header=False)\n",
    "\n",
    "    # 4.5 Quit\n",
    "    end =datetime.now() \n",
    "    print('Code Runned No Problem')\n",
    "    print(f'Time = {end - start}')\n",
    "    sleep(5)\n",
    "    driver.quit()\n",
    "\n",
    "    df.head()\n",
    "    \n",
    "end_01 =datetime.now()\n",
    "print(f'Total Time: {end_01 - start_01}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed0c7c7-9ab7-485e-a134-141bccf0155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "675b907e-82b6-40b7-97c7-77d15d98089e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Business Analyst\n",
      "2\n",
      "Data Scientist\n",
      "3\n",
      "Data Analyst\n"
     ]
    }
   ],
   "source": [
    "job_list = ['Business Analyst', 'Data Scientist', 'Data Analyst' ]\n",
    "n=0\n",
    "\n",
    "while n<3:\n",
    "    n+=1\n",
    "    job_name = job_list[n-1]\n",
    "    print(n)\n",
    "    print(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e007d99b-d2d7-4cdc-a2ed-57c810fdb2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macbook/Desktop/projects/Github_Repositories/Portfolio Projects/02 - Web_Scraping_Job_Search'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79f6803-bbb4-481c-aec4-701e30a9e105",
   "metadata": {},
   "source": [
    "# Links with searching status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9dd81a-2d93-4d02-a614-1d4dedd2582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.stepstone.de/jobs/data-engineer/in-rietberg?radius=100\n",
    "\n",
    "https://www.stepstone.de/jobs/data-engineer/in-rietberg?radius=100&page=1&sort=2&action=sort_publish\n",
    "\n",
    "https://www.stepstone.de/jobs/data-engineer/in-rietberg?radius=100&page=2&sort=2&action=sort_publish\n",
    "\n",
    "https://www.stepstone.de/jobs/data-engineer/in-rietberg?radius=100&page=2&sort=1&action=facet_selected%3bage%3bage_7&ag=age_7 # son 1 hafta\n",
    "\n",
    "https://www.stepstone.de/jobs/data-engineer/in-rietberg?radius=100&page=1&sort=1&action=facet_selected%3bage%3bage_1&ag=age_1 # son 24 saat\n",
    "\n",
    "\n",
    "https://www.xing.com/jobs/search?keywords=Data%20Engineer&location=Rietberg&radius=100&sort=relevance \n",
    "\n",
    "https://www.xing.com/jobs/search?keywords=Data%20Engineer&location=Rietberg&radius=100&sort=date # newest first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74f9cf1c-ba3f-47d1-8256-293b6c76ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Element Function\n",
    "def find_element(H):\n",
    "    header = driver.find_elements(By.CLASS_NAME, H)\n",
    "    list_header = [title.text for title in header]\n",
    "    return list_header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c8c74d-e06b-4d6e-9bbb-5550555aa081",
   "metadata": {},
   "source": [
    "# Xing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "577f5fdf-7c16-48ef-ab7c-60f5c8dd80c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- Xing Job Searching Selenium Project ----------------------\n",
      "DataFrame End : (20, 9)\n",
      "[40, 20, 20, 20, 20]\n",
      "Finish 2023-03-10 09:04:55\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>publish</th>\n",
       "      <th>company</th>\n",
       "      <th>city</th>\n",
       "      <th>description</th>\n",
       "      <th>link</th>\n",
       "      <th>website</th>\n",
       "      <th>date</th>\n",
       "      <th>search_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Software Engineer (m/w/d)</td>\n",
       "      <td>Vor 7 Stunden</td>\n",
       "      <td>inserve GmbH</td>\n",
       "      <td>Osnabrück</td>\n",
       "      <td>Engineering und Data Streaming Dein Spielfeld?</td>\n",
       "      <td>https://www.xing.com/jobs/osnabrueck-senior-so...</td>\n",
       "      <td>xing</td>\n",
       "      <td>2023-03-10 09:04:55</td>\n",
       "      <td>Data Engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lead Engineer, Powertrain Verification</td>\n",
       "      <td>Vor 8 Stunden</td>\n",
       "      <td>Vestas Wind Systems A/S</td>\n",
       "      <td>Dortmund</td>\n",
       "      <td>Responsibilities As Lead Engineer - Powertrain...</td>\n",
       "      <td>https://www.xing.com/jobs/dortmund-lead-engine...</td>\n",
       "      <td>xing</td>\n",
       "      <td>2023-03-10 09:04:55</td>\n",
       "      <td>Data Engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Engineer Sports (all genders)</td>\n",
       "      <td>Vor 15 Stunden</td>\n",
       "      <td>adesso SE</td>\n",
       "      <td>Dortmund</td>\n",
       "      <td>Pipelines, Data Ingest und Date Processing.</td>\n",
       "      <td>https://www.xing.com/jobs/dortmund-data-engine...</td>\n",
       "      <td>xing</td>\n",
       "      <td>2023-03-10 09:04:55</td>\n",
       "      <td>Data Engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Architect (all genders)</td>\n",
       "      <td>Vor 15 Stunden</td>\n",
       "      <td>adesso SE</td>\n",
       "      <td>Dortmund</td>\n",
       "      <td>Das Competence Center Data Engineering ist ver...</td>\n",
       "      <td>https://www.xing.com/jobs/dortmund-data-archit...</td>\n",
       "      <td>xing</td>\n",
       "      <td>2023-03-10 09:04:55</td>\n",
       "      <td>Data Engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Engineer (all genders)</td>\n",
       "      <td>Vor 15 Stunden</td>\n",
       "      <td>adesso SE</td>\n",
       "      <td>Münster</td>\n",
       "      <td>Als Data Engineer bist du Teil eines innovativ...</td>\n",
       "      <td>https://www.xing.com/jobs/muenster-data-engine...</td>\n",
       "      <td>xing</td>\n",
       "      <td>2023-03-10 09:04:55</td>\n",
       "      <td>Data Engineer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                job_title         publish  \\\n",
       "0        Senior Software Engineer (m/w/d)   Vor 7 Stunden   \n",
       "1  Lead Engineer, Powertrain Verification   Vor 8 Stunden   \n",
       "2      Data Engineer Sports (all genders)  Vor 15 Stunden   \n",
       "3            Data Architect (all genders)  Vor 15 Stunden   \n",
       "4             Data Engineer (all genders)  Vor 15 Stunden   \n",
       "\n",
       "                   company       city  \\\n",
       "0             inserve GmbH  Osnabrück   \n",
       "1  Vestas Wind Systems A/S   Dortmund   \n",
       "2                adesso SE   Dortmund   \n",
       "3                adesso SE   Dortmund   \n",
       "4                adesso SE    Münster   \n",
       "\n",
       "                                         description  \\\n",
       "0     Engineering und Data Streaming Dein Spielfeld?   \n",
       "1  Responsibilities As Lead Engineer - Powertrain...   \n",
       "2        Pipelines, Data Ingest und Date Processing.   \n",
       "3  Das Competence Center Data Engineering ist ver...   \n",
       "4  Als Data Engineer bist du Teil eines innovativ...   \n",
       "\n",
       "                                                link website  \\\n",
       "0  https://www.xing.com/jobs/osnabrueck-senior-so...    xing   \n",
       "1  https://www.xing.com/jobs/dortmund-lead-engine...    xing   \n",
       "2  https://www.xing.com/jobs/dortmund-data-engine...    xing   \n",
       "3  https://www.xing.com/jobs/dortmund-data-archit...    xing   \n",
       "4  https://www.xing.com/jobs/muenster-data-engine...    xing   \n",
       "\n",
       "                  date   search_title  \n",
       "0  2023-03-10 09:04:55  Data Engineer  \n",
       "1  2023-03-10 09:04:55  Data Engineer  \n",
       "2  2023-03-10 09:04:55  Data Engineer  \n",
       "3  2023-03-10 09:04:55  Data Engineer  \n",
       "4  2023-03-10 09:04:55  Data Engineer  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('---------------------- Xing Job Searching Selenium Project ----------------------')\n",
    "start=datetime.now()  \n",
    "# Link Descriptions\n",
    "link_original_xing = 'https://www.xing.com/jobs/search?keywords=Data%20Engineer&location=Rietberg&page=1&radius=100'\n",
    "\n",
    "website_name = 'xing'\n",
    "job_name = 'Data Engineer'\n",
    "#job_name = 'Data Analyst'\n",
    "#job_name = 'Data Scientist'\n",
    "ort_ = 'Rietberg'\n",
    "radius = 50\n",
    "page_number = 1\n",
    "\n",
    "#  1 - Create Driver\n",
    "Path = '/Users/macbook/Desktop/projects/Github_Repositories/Portfolio Projects/02 - Web_Scraping_Job_Search/chromedriver'\n",
    "driver = webdriver.Chrome(Path)\n",
    "\n",
    "#  2 - Go to Website\n",
    "job_link = job_name.replace(' ', '-').lower()\n",
    "ort_link = ort_.lower()\n",
    "link = f'https://www.xing.com/jobs/search?keywords=Data%20Engineer&location=Rietberg&page=1&radius=100&sort=date'\n",
    "\n",
    "driver.get(link)\n",
    "wait(10)\n",
    "sleep(2)\n",
    "\n",
    "#  3 - ActionChain Object created\n",
    "# 3.1 - Click Banned Accept\n",
    "ID = 'consent-accept-button'\n",
    "click_bann_byID(ID)\n",
    "\n",
    "# 4 -  Take Infos from Page\n",
    "# 4.1 - Headers, Publish_Time ,Company, City\n",
    "H = 'utils-line-clamp-lineClamp2-dfe26aab'\n",
    "D = 'list-item-job-teaser-list-item-highlight-bb8ddbb6'\n",
    "L = 'list-item-job-teaser-list-item-location-a5b28738'\n",
    "ALL = 'list-item-job-teaser-list-item-listItem-f04c772e'\n",
    "\n",
    "\n",
    "list_header = find_element(H)\n",
    "list_description = find_element(D)\n",
    "list_ort = find_element(L)\n",
    "list_all = find_element(ALL)\n",
    "\n",
    "list_publish = []\n",
    "list_full_time = [] \n",
    "for i in list_all:\n",
    "    date = i.split('\\n')[-2]\n",
    "    time_ = i.split('\\n')[-3]\n",
    "    list_publish.append(date)\n",
    "    list_full_time.append(time_)\n",
    "\n",
    "list_title =[]\n",
    "list_company = []\n",
    "n = 0\n",
    "while n < len(list_header):\n",
    "    list_title.append(list_header[n])\n",
    "    list_company.append(list_header[n+1])\n",
    "    n += 2\n",
    "\n",
    "# 4.3 - Get Links\n",
    "Link = 'list-item-job-teaser-list-item-listItem-f04c772e'\n",
    "header = driver.find_elements(By.CLASS_NAME, Link)\n",
    "list_link = [link.get_attribute('href') for link in header]\n",
    "\n",
    "# 4.4 - DataFrame df\n",
    "d = dict(job_title=np.array(list_title), publish=np.array(list_publish), company=np.array(list_company), city=np.array(list_ort) , description=np.array(list_description), link=np.array(list_link))\n",
    "df = pd.DataFrame.from_dict(d, orient='index')\n",
    "df = df.T\n",
    "df['website'] = website_name\n",
    "time_now = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "df['date'] = time_now\n",
    "df['search_title'] = job_name\n",
    "\n",
    "\n",
    "# 5.1 - Save Data as csv \n",
    "print(f'DataFrame End : {df.shape}')\n",
    "path = '/Users/macbook/Desktop/projects/Github_Repositories/Portfolio Projects/02 - Web_Scraping_Job_Search/data'\n",
    "df.to_csv(f'{path}/{job_name}.csv', mode='a', index=False, header=False)\n",
    "\n",
    "list_of_list = [list_header, list_description, list_ort, list_publish, list_link]\n",
    "print([len(i) for i in list_of_list])\n",
    "\n",
    "sleep(2)\n",
    "driver.quit()\n",
    "print('Finish', time_now)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d863fddf-c358-4541-a7eb-3c506bb5ba3b",
   "metadata": {},
   "source": [
    "# Connect Database POSTGRESQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d147a67-6218-4403-a838-4f3944807bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Brunel',)\n",
      "('Campusjäger by Workwise',)\n",
      "('Campusjäger by Workwise',)\n",
      "('Campusjäger by Workwise',)\n",
      "('Campusjäger by Workwise',)\n",
      "('Campusjäger by Workwise',)\n",
      "('Campusjäger by Workwise',)\n",
      "('Campusjäger by Workwise',)\n",
      "('Campusjäger by Workwise',)\n",
      "('Campusjäger by Workwise',)\n",
      "('Campusjäger by Workwise',)\n",
      "('Vesterling AG',)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(database=\"JOB\",\n",
    "\t\t\t\t\t\tuser='postgres', password=1984,\n",
    "\t\t\t\t\t\thost='127.0.0.1', port='5432'\n",
    ")\n",
    "\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "sql = '''CREATE TABLE IF NOT EXISTS dataeng(job_title varchar(300) NOT NULL,\\\n",
    "publish varchar(30),\\\n",
    "company varchar(300),\\\n",
    "city varchar(300),\\\n",
    "description varchar(300),\\\n",
    "link varchar(300),\\\n",
    "website varchar(30),\\\n",
    "date timestamp,\\\n",
    "search_title varchar(20));'''\n",
    "\n",
    "cursor.execute(sql)\n",
    "\n",
    "#### conda install -c anaconda sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "# connection string: driver://username:password@server/database\n",
    "engine = create_engine('postgresql+psycopg2://postgres:1984@localhost/JOB')\n",
    "\n",
    "#  Note:  if_exists can be append, replace, fail.  \n",
    "df.to_sql('dataeng', engine, if_exists='append', index = False)\n",
    "\n",
    "\n",
    "sql2 = '''select company from dataeng Where publish LIKE '%hours%' '''\n",
    "cursor.execute(sql2)\n",
    "for i in cursor.fetchall():\n",
    "\tprint(i)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56679949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-2.0.5.post1-cp310-cp310-macosx_10_9_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=4.2.0\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-2.0.2-cp310-cp310-macosx_11_0_x86_64.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: typing-extensions, greenlet, sqlalchemy\n",
      "Successfully installed greenlet-2.0.2 sqlalchemy-2.0.5.post1 typing-extensions-4.5.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "34740bd4-9998-49fa-9fdc-fdb4d12566fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.xing.com/jobs/dortmund-data-engineer-98044745'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_link[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f93be27-203a-4ce0-8443-d957a85e4611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Junior Software Developer / Data Engineer (m/w/d)\\nDortmund\\nMotionMiners GmbH\\n€53,500 – €69,500\\nFull-time\\nYesterday\\nSave job'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_link[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "91f5302a-399d-4854-9b44-6b385f66a022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Engineer - Data Warehouse / Datenpflege / Home Office (m/w/d)\\nBochum\\nCampusjäger by Workwise\\n4.7\\nFull-time\\nEngineer - Data Warehouse / Datenpflege / Home Office (m/w/d) klingt vielversprechend?\\n20 hours ago\\nSave job'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_link[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "96cc338a-3125-495a-aaf2-429c7539e6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.xing.com/jobs/bochum-data-engineer-data-warehouse-datenpflege-home-office-98097563'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_link[2].split('\\n')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c3790e52-e450-4c44-8756-dd204630dcf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Engineer (m/w/d)',\n",
       " 'Dortmund',\n",
       " 'BRUDERKOPF GmbH & Co. KG',\n",
       " 'Full-time',\n",
       " 'Für ihn suchen wir einen Data Engineer (m/w/d) . Wahlweise ist die Arbeit am Standort unseres Mandanten oder remote möglich.',\n",
       " '2 days ago',\n",
       " 'Save job']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_link[0].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1f34c3a3-aaf3-4ed5-bde5-c22d14ea7a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     https://www.xing.com/jobs/muenster-senior-data...\n",
       "1     https://www.xing.com/jobs/dortmund-senior-data...\n",
       "2     https://www.xing.com/jobs/paderborn-senior-dat...\n",
       "3     https://www.xing.com/jobs/gelsenkirchen-big-da...\n",
       "4     https://www.xing.com/jobs/bochum-big-data-deve...\n",
       "5     https://www.xing.com/jobs/bielefeld-big-data-d...\n",
       "6     https://www.xing.com/jobs/muenster-big-data-de...\n",
       "7     https://www.xing.com/jobs/dortmund-big-data-de...\n",
       "8     https://www.xing.com/jobs/kassel-senior-data-e...\n",
       "9     https://www.xing.com/jobs/kassel-it-data-engin...\n",
       "10    https://www.xing.com/jobs/dortmund-architect-d...\n",
       "11    https://www.xing.com/jobs/muenster-cloud-archi...\n",
       "12    https://www.xing.com/jobs/paderborn-cloud-arch...\n",
       "13    https://www.xing.com/jobs/dortmund-cloud-archi...\n",
       "14    https://www.xing.com/jobs/paderborn-system-eng...\n",
       "15    https://www.xing.com/jobs/paderborn-sas-admini...\n",
       "16    https://www.xing.com/jobs/dortmund-sas-adminis...\n",
       "17    https://www.xing.com/jobs/paderborn-linux-syst...\n",
       "18    https://www.xing.com/jobs/dortmund-linux-syste...\n",
       "19    https://www.xing.com/jobs/selm-house-dwh-data-...\n",
       "Name: link, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5efec7a4-5524-4074-9fc7-2ba0620b1b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'James', 12000, 'D1')\n",
      "(2, 'Cellim', 20000, 'D1')\n",
      "(3, 'Abdullah', 18000, 'D3')\n"
     ]
    }
   ],
   "source": [
    "insert_values = [(1, 'James', 12000, 'D1'), (2, 'Cellim', 20000, 'D1'), (3, 'Abdullah', 18000, 'D3')]\n",
    "\n",
    "for record in insert_values:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "39aa5a9c-f38e-4367-8e49-50d44194a970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Engineer (m/w/d)\n",
      "7 hours ago\n",
      "Brunel\n",
      "Osnabrück\n",
      "Ihre Aufgabe Als Data Engineer entwickeln Sie Data Management Lösungen im Anwendungsspezifischen Bereich unseres Kunden.\n",
      "https://www.xing.com/jobs/osnabrueck-data-engineer-98150533\n",
      "xing\n",
      "2023-03-08-09:00:09\n",
      "Data Engineer\n"
     ]
    }
   ],
   "source": [
    "for i in df.iloc[0]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ca5401e5-4f0c-4618-9eaa-ade743188f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job_title                                   Data Engineer (m/w/d)\n",
       "publish                                               7 hours ago\n",
       "company                                                    Brunel\n",
       "city                                                    Osnabrück\n",
       "description     Ihre Aufgabe Als Data Engineer entwickeln Sie ...\n",
       "link            https://www.xing.com/jobs/osnabrueck-data-engi...\n",
       "website                                                      xing\n",
       "date                                          2023-03-08-09:00:09\n",
       "search_title                                        Data Engineer\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525d1a8-883a-4aea-b32a-54b18fe64e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
