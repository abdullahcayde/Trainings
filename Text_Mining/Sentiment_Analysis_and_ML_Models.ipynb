{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a80038b-0488-4c62-ba9f-257f05dd1da5",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f9762a-17bb-4134-8264-130657b5530f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-14 11:51:17.708820: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from textblob import TextBlob\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a49f1a-a112-4cf5-b1d6-e657b01fc475",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc72cb3c-5833-4d09-82c7-a5df2c93d87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>series demonstrating adage good goose also goo...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>good goose</td>\n",
       "      <td>pozitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>good</td>\n",
       "      <td>pozitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>gander occasionally amuses none amount much story</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>amuses</td>\n",
       "      <td>pozitive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text     label\n",
       "0   series demonstrating adage good goose also goo...  negative\n",
       "21                                         good goose  pozitive\n",
       "22                                               good  pozitive\n",
       "33  gander occasionally amuses none amount much story  negative\n",
       "46                                             amuses  pozitive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Recap 01\n",
    "data = pd.read_csv('film_comments.tsv', sep='\\t')\n",
    "\n",
    "# Change 0,1,3,4 to negatif and pozitif\n",
    "data.Sentiment.replace(0, 'negative', inplace=True)\n",
    "data.Sentiment.replace(1, 'negative', inplace=True)\n",
    "\n",
    "data.Sentiment.replace(3, 'pozitive', inplace=True)\n",
    "data.Sentiment.replace(4, 'pozitive', inplace=True)\n",
    "\n",
    "# Select only Negatif and Pozitif, remove 2 Sentiments\n",
    "data = data[(data.Sentiment == 'negative') | (data.Sentiment == 'pozitive')]\n",
    "\n",
    "# Df DataFrame\n",
    "df = pd.DataFrame()\n",
    "df['text'] = data.Phrase\n",
    "df['label'] = data.Sentiment\n",
    "\n",
    "# LOWER \n",
    "df['text'] = df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "# Punctuation Marks\n",
    "df['text'] = df['text'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Digits\n",
    "df['text'] = df['text'].str.replace('\\d','')\n",
    "\n",
    "# Stopwords\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n",
    "\n",
    "# Low Frequency Word\n",
    "delete = pd.Series(' '.join(df['text']).split()).value_counts()[-1000:]\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in delete))\n",
    "\n",
    "#lemmi\n",
    "from textblob import Word\n",
    "#nltk.download('wordnet')\n",
    "df['text'] = df['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])) \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af366a56-0a13-4da6-8d90-19ca3d285b11",
   "metadata": {},
   "source": [
    "# Test - Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10bf4b7e-0627-4084-976f-a4f5a4c37ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-Train Split\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(df[\"text\"],\n",
    "                                                                   df[\"label\"], \n",
    "                                                                    random_state = 1)\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6630b415-e25c-4e07-80fe-7eec84b178fb",
   "metadata": {},
   "source": [
    "# Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b083eda-7ed3-40e8-8af3-8d8478bcc190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectors\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_x)\n",
    "\n",
    "x_train_count = vectorizer.transform(train_x)\n",
    "x_test_count = vectorizer.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9964ba91-99c6-404e-b430-5e247f2e637a",
   "metadata": {},
   "source": [
    "# TF - IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c029d77d-3040-47fc-8e39-7279fe3a0d7f",
   "metadata": {},
   "source": [
    "## World Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65d2ca71-6469-4e79-86f5-c4351086d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_word_vectorizer = TfidfVectorizer()\n",
    "tf_idf_word_vectorizer.fit(train_x)\n",
    "\n",
    "x_train_tf_idf_word = tf_idf_word_vectorizer.transform(train_x)\n",
    "x_test_tf_idf_word = tf_idf_word_vectorizer.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2514897-7e4a-44e7-bf63-c99642b29695",
   "metadata": {},
   "source": [
    "## N-Gram Level tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8725700b-2a2f-4b5c-8f35-3a545f951d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_ngram_vectorizer = TfidfVectorizer(ngram_range = (2,3))\n",
    "tf_idf_ngram_vectorizer.fit(train_x)\n",
    "\n",
    "x_train_tf_idf_ngram = tf_idf_ngram_vectorizer.transform(train_x)\n",
    "x_test_tf_idf_ngram = tf_idf_ngram_vectorizer.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da3375-71ba-4831-a8dc-4df8940bdb65",
   "metadata": {},
   "source": [
    "## Character Level tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b37953c-a2fd-4bd1-9bd4-3b1705b41b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_chars_vectorizer = TfidfVectorizer(analyzer = \"char\", ngram_range = (2,3))\n",
    "tf_idf_chars_vectorizer.fit(train_x)\n",
    "\n",
    "x_train_tf_idf_chars = tf_idf_chars_vectorizer.transform(train_x)\n",
    "x_test_tf_idf_chars = tf_idf_chars_vectorizer.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f56d6-26a1-4f4d-9a93-f5b002d1f10e",
   "metadata": {},
   "source": [
    "# Machine Learning Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc3b4d-01e7-428a-b2f4-b0c25f830091",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08fb2768-4593-4adb-99a1-38869ab0f782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectors Accuracy Ratio: 0.8368200836820083\n"
     ]
    }
   ],
   "source": [
    "loj = linear_model.LogisticRegression()\n",
    "loj_model_count = loj.fit(x_train_count, train_y)\n",
    "accuracy_loj_count = model_selection.cross_val_score(loj_model_count, \n",
    "                                           x_test_count, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"Count Vectors Accuracy Ratio:\", accuracy_loj_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a74c822-34ba-4b96-ba70-f99c0ce04f04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-Level TF-IDF Accuracy Ratio: 0.8331589958158995\n"
     ]
    }
   ],
   "source": [
    "loj = linear_model.LogisticRegression()\n",
    "loj_model_word = loj.fit(x_train_tf_idf_word,train_y)\n",
    "accuracy_loj_word = model_selection.cross_val_score(loj_model_word, \n",
    "                                           x_test_tf_idf_word, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"Word-Level TF-IDF Accuracy Ratio:\", accuracy_loj_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9f17a97-8b93-4747-a766-3bdeb9c73da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM TF-IDF Accuracy Ratio: 0.748326359832636\n"
     ]
    }
   ],
   "source": [
    "loj = linear_model.LogisticRegression()\n",
    "loj_model_ngram = loj.fit(x_train_tf_idf_ngram,train_y)\n",
    "accuracy_loj_ngram = model_selection.cross_val_score(loj_model_ngram, \n",
    "                                           x_test_tf_idf_ngram, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"N-GRAM TF-IDF Accuracy Ratio:\", accuracy_loj_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28e76fb9-36f7-4c9d-b238-1186227ebdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARLEVEL Accuracy Ratio: 0.7811715481171548\n"
     ]
    }
   ],
   "source": [
    "loj = linear_model.LogisticRegression()\n",
    "loj_model_char = loj.fit(x_train_tf_idf_chars,train_y)\n",
    "accuracy_loj_char = model_selection.cross_val_score(loj_model_char, \n",
    "                                           x_test_tf_idf_chars, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"CHARLEVEL Accuracy Ratio:\", accuracy_loj_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c63b3c-2ac0-4d52-ad25-2b5b8fbf7340",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab97502d-514a-49e8-8eee-11fac5e09229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectors Accuracy Ratio: 0.8332112970711296\n"
     ]
    }
   ],
   "source": [
    "nb = naive_bayes.MultinomialNB()\n",
    "nb_model_count = nb.fit(x_train_count,train_y)\n",
    "accuracy_nb_count = model_selection.cross_val_score(nb_model_count, \n",
    "                                           x_test_count, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"Count Vectors Accuracy Ratio:\", accuracy_nb_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a623797b-379b-41fa-8607-9c25ed8eb8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-Level TF-IDF Accuracy Ratio: 0.835041841004184\n"
     ]
    }
   ],
   "source": [
    "nb = naive_bayes.MultinomialNB()\n",
    "nb_model_word = nb.fit(x_train_tf_idf_word,train_y)\n",
    "accuracy_nb_word = model_selection.cross_val_score(nb_model_word, \n",
    "                                           x_test_tf_idf_word, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"Word-Level TF-IDF Accuracy Ratio:\", accuracy_nb_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a0a09ba-bc51-448d-97e1-26b383bca690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM TF-IDF Accuracy Ratio: 0.7685146443514643\n"
     ]
    }
   ],
   "source": [
    "nb = naive_bayes.MultinomialNB()\n",
    "nb_model_ngram = nb.fit(x_train_tf_idf_ngram,train_y)\n",
    "accuracy_nb_ngram = model_selection.cross_val_score(nb_model_ngram, \n",
    "                                           x_test_tf_idf_ngram, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"N-GRAM TF-IDF Accuracy Ratio:\", accuracy_nb_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7b570da-85c8-44fc-ba15-4d2484001f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARLEVEL Accuracy Ratio: 0.7557008368200837\n"
     ]
    }
   ],
   "source": [
    "nb = naive_bayes.MultinomialNB()\n",
    "nb_model_char = nb.fit(x_train_tf_idf_chars,train_y)\n",
    "accuracy_nb_char = model_selection.cross_val_score(nb_model_char, \n",
    "                                           x_test_tf_idf_chars, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"CHARLEVEL Accuracy Ratio:\", accuracy_nb_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11707dae-851f-4fae-9277-9d7dab611ba8",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b81d6ee0-8d01-4955-8d72-785a0e2120d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf = ensemble.RandomForestClassifier()\n",
    "#rf_model = rf.fit(x_train_count,train_y)\n",
    "#accuracy = model_selection.cross_val_score(rf_model, \n",
    "#                                           x_test_count, \n",
    "#                                           test_y, \n",
    "#                                           cv = 10).mean()\n",
    "\n",
    "#print(\"Count Vectors Doğruluk Oranı:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cc962c7-446d-40f9-93f9-9717f30f25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf = ensemble.RandomForestClassifier()\n",
    "#rf_model = rf.fit(x_train_tf_idf_word,train_y)\n",
    "#accuracy = model_selection.cross_val_score(rf_model, \n",
    "#                                           x_test_tf_idf_word, \n",
    "#                                           test_y, \n",
    "#                                           cv = 10).mean()\n",
    "\n",
    "#print(\"Word-Level TF-IDF Accuracy Ratio:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6e8e502-0602-47ba-aec4-b37ae4397256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf = ensemble.RandomForestClassifier()\n",
    "#rf_model = loj.fit(x_train_tf_idf_ngram,train_y)\n",
    "#accuracy = model_selection.cross_val_score(rf_model, \n",
    "#                                           x_test_tf_idf_ngram, \n",
    "#                                           test_y, \n",
    "#                                           cv = 10).mean()\n",
    "\n",
    "#print(\"N-GRAM TF-IDF Accuracy Ratio:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75494a8f-0e7d-4655-a3a2-4d743f510976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf = ensemble.RandomForestClassifier()\n",
    "#rf_model = loj.fit(x_train_tf_idf_chars,train_y)\n",
    "#accuracy = model_selection.cross_val_score(rf_model, \n",
    "#                                           x_test_tf_idf_chars, \n",
    "#                                           test_y, \n",
    "#                                           cv = 10).mean()\n",
    "\n",
    "#print(\"CHARLEVEL Accuracy Ratio:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6648cda6-bba6-4b0e-8890-c2f115c093e1",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2deea8c-1449-4ebf-a4e1-731d8624d460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectors Accuracy Ratio: 0.7153242677824267\n"
     ]
    }
   ],
   "source": [
    "xgb = xgboost.XGBClassifier()\n",
    "xgb_model_count = xgb.fit(x_train_count,train_y)\n",
    "accuracy_xgb_count = model_selection.cross_val_score(xgb_model_count, \n",
    "                                           x_test_count, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"Count Vectors Accuracy Ratio:\", accuracy_xgb_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02d0ff25-b9e1-474e-ac85-dfbaa66ba8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-Level TF-IDF Accuracy Ratio: 0.7080020920502091\n"
     ]
    }
   ],
   "source": [
    "xgb = xgboost.XGBClassifier()\n",
    "xgb_model_word = xgb.fit(x_train_tf_idf_word,train_y)\n",
    "accuracy_xgb_word = model_selection.cross_val_score(xgb_model_word, \n",
    "                                           x_test_tf_idf_word, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"Word-Level TF-IDF Accuracy Ratio:\", accuracy_xgb_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e0b89e2-aa4f-453e-9a87-d969e80e41f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM TF-IDF Accuracy Ratio: 0.5827928870292888\n"
     ]
    }
   ],
   "source": [
    "xgb = xgboost.XGBClassifier()\n",
    "xgb_model_ngram = xgb.fit(x_train_tf_idf_ngram,train_y)\n",
    "accuracy_xgb_ngram = model_selection.cross_val_score(xgb_model_ngram, \n",
    "                                           x_test_tf_idf_ngram, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"N-GRAM TF-IDF Accuracy Ratio:\", accuracy_xgb_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2b093e6-d517-4f69-82a9-bc5c05fece10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARLEVEL Accuracy Ratio: 0.7783472803347281\n"
     ]
    }
   ],
   "source": [
    "xgb = xgboost.XGBClassifier()\n",
    "xgb_model_char = xgb.fit(x_train_tf_idf_chars,train_y)\n",
    "accuracy_xgb_char = model_selection.cross_val_score(xgb_model_char, \n",
    "                                           x_test_tf_idf_chars, \n",
    "                                           test_y, \n",
    "                                           cv = 10).mean()\n",
    "\n",
    "print(\"CHARLEVEL Accuracy Ratio:\", accuracy_xgb_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406409da-1719-4e7c-8072-abf3da65c7d0",
   "metadata": {},
   "source": [
    "## Compare ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7dbf6e3-8bf3-4a5b-9b4f-ac9db35e12ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Logistic_Reg_Acc</th>\n",
       "      <th>NavieBayes_Acc</th>\n",
       "      <th>XGB_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>0.836820</td>\n",
       "      <td>0.833211</td>\n",
       "      <td>0.715324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word</td>\n",
       "      <td>0.833159</td>\n",
       "      <td>0.833159</td>\n",
       "      <td>0.708002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n_gram</td>\n",
       "      <td>0.748326</td>\n",
       "      <td>0.768515</td>\n",
       "      <td>0.582793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>character</td>\n",
       "      <td>0.781172</td>\n",
       "      <td>0.755701</td>\n",
       "      <td>0.778347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Type  Logistic_Reg_Acc  NavieBayes_Acc   XGB_Acc\n",
       "0      count          0.836820        0.833211  0.715324\n",
       "1       word          0.833159        0.833159  0.708002\n",
       "2     n_gram          0.748326        0.768515  0.582793\n",
       "3  character          0.781172        0.755701  0.778347"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'Type' : ['count', 'word', 'n_gram', 'character'],\n",
    "     'Logistic_Reg_Acc' : [accuracy_loj_count, accuracy_loj_word, accuracy_loj_ngram, accuracy_loj_char],\n",
    "     'NavieBayes_Acc' : [accuracy_nb_count, accuracy_loj_word, accuracy_nb_ngram, accuracy_nb_char],\n",
    "     'XGB_Acc' : [accuracy_xgb_count, accuracy_xgb_word, accuracy_xgb_ngram, accuracy_xgb_char]}\n",
    "\n",
    "df_compare = pd.DataFrame(d )\n",
    "df_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd04279-2568-481b-bab8-b00df94eccc1",
   "metadata": {},
   "source": [
    "# Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f9a7e8e-1c55-4bab-8843-b2a0629277e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_comment01 = 'yes i like this film'\n",
    "\n",
    "# With Count Vector and Lojistic Reg.\n",
    "loj = linear_model.LogisticRegression()\n",
    "loj_model = loj.fit(x_train_count, train_y)\n",
    "\n",
    "v = CountVectorizer()\n",
    "v.fit(train_x)\n",
    "\n",
    "new_comment01 = pd.Series(new_comment01)\n",
    "new_comment01 = v.transform(new_comment01)\n",
    "loj_model_count.predict(new_comment01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3963e09-7d57-4ece-bd2f-62a18416df52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n",
      "[0]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# With Count Vector and Lojistic Reg.\n",
    "new_comment01 = 'yes i like this film'\n",
    "new_comment02 = 'this film is very nice and good i like it'\n",
    "new_comment03 = 'no not good look at that shit very bad'\n",
    "new_comment04 = 'not a good idea'\n",
    "\n",
    "L = [new_comment01, new_comment02, new_comment03, new_comment04]\n",
    "results = list(filter(lambda x: print(loj_model_count.predict(v.transform(pd.Series(x)))), L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0489f92-0f68-4a5d-a7a2-a150bc1353df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# With Ngram Vector and Lojistic Reg.\n",
    "new_comment01 = pd.Series('yes i like this film')\n",
    "new_comment02 = pd.Series('this film is very nice and good i like it') \n",
    "new_comment03 = pd.Series('no not good look at that shit very bad')\n",
    "new_comment04 = pd.Series('not a good idea')\n",
    "\n",
    "L = [new_comment01, new_comment02, new_comment03, new_comment04]\n",
    "\n",
    "v = TfidfVectorizer(ngram_range = (2,3))\n",
    "v.fit(train_x)\n",
    "\n",
    "results = list(filter(lambda x: print(loj_model_ngram.predict(v.transform(x))), L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "688202ae-1556-47a2-b7c0-105a5324f1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n",
      "[0]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# With Word Vector and Lojistic Reg.\n",
    "new_comment01 = pd.Series('yes i like this film')\n",
    "new_comment02 = pd.Series('this film is very nice and good i like it') \n",
    "new_comment03 = pd.Series('no not good look at that shit very bad')\n",
    "new_comment04 = pd.Series('not good')\n",
    "\n",
    "L = [new_comment01, new_comment02, new_comment03, new_comment04]\n",
    "\n",
    "v = TfidfVectorizer()\n",
    "v.fit(train_x)\n",
    "\n",
    "results = list(filter(lambda x: print(loj_model_word.predict(v.transform(x))), L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a27db52-5b09-4f06-b576-3abb8a1c0097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba69a799-b3f3-4f06-bab7-c3b0d88c5268",
   "metadata": {},
   "source": [
    "## Ready Sentiment Analysis on https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b84e99-8c47-4ac1-b314-37c307c56e96",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc081be1-3da9-4b61-9f91-6b0156cd1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "data = [\"I love you\", \"I hate you\"]\n",
    "data2 = ['yes i like this film',\n",
    "         'this film is very nice and good i like it',\n",
    "         'no not good look at that shit very bad',\n",
    "         'not good']\n",
    "\n",
    "sentiment_pipeline(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f72f9-a567-4329-9cd0-95c87b40bdea",
   "metadata": {},
   "source": [
    "## German (positive - negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a2921-2a29-4629-8b9f-402fa0fa124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from germansentiment import SentimentModel\n",
    "\n",
    "model = SentimentModel()\n",
    "\n",
    "texts = [\n",
    "    \"Mit keinem guten Ergebniss\",\"Das ist gar nicht mal so gut\",\n",
    "    \"Total awesome!\",\"nicht so schlecht wie erwartet\",\n",
    "    \"Der Test verlief positiv.\",\"Sie fährt ein grünes Auto.\"]\n",
    "       \n",
    "result = model.predict_sentiment(texts)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed7de6-210a-45ef-be56-bf7d10cadb30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
